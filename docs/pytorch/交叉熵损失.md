torch.nn.functional.cross_entropy(input, target, weight=**None**, size_average=**True**)

该函数使用了 log_softmax 和 nll_loss，详细请看CrossEntropyLoss（类的形式）

![img](https://raw.githubusercontent.com/gfjiangly/knowledges/master/assets/log_softmax.png)

虽然在数学上log_softmax等价于log(softmax(x))，但做这两个单独操作速度较慢，数值上也不稳定。这个函数使用另一种公式来正确计算输出和梯度。

 

参数：

input - (N,C) 其中，C 是类别的个数

target - (N) 其大小是 0 <= targets[i] <= C-1 （注意不是one-hot编码）

weight (Variable, optional) – 一个可手动指定每个类别的权重。如果给定的话，必须是大小为nclasses的Variable 

size_average (bool, optional) – 默认情况下，是mini-batch loss的平均值，然而，如果size_average=False，则是mini-batch loss的总和

 

源码
```python
@torch._jit_internal.weak_script
def cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean'):

    if size_average is not None or reduce is not None:
        reduction = _Reduction.legacy_get_string(size_average, reduce)
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)

```


![img](https://raw.githubusercontent.com/gfjiangly/knowledges/master/assets/torch.nn.CrossEntropyLoss.png)

 

举例：
```python
import torch
import torch.nn.functional as F
from torch.autograd import Variable


x = Variable(torch.Tensor([[1.0,2.0,3.0], [1.0,2.0,3.0]]))
y = Variable(torch.LongTensor([1, 2]))
w = torch.Tensor([1.0,1.0,1.0])
res = F.cross_entropy(x,y,w)
# 0.9076

w = torch.Tensor([1.0,10.0,1.0])
res = F.cross_entropy(x,y,w)
# 1.3167

```

默认是有平均的，平均是根据weight参数加权的，如1.3167=(10*1.4076+1*0.4076)/(10+1)

也可以先将weight归一化，后面可以自己平均（如ssd.pytorch代码），结果都是一样的。

 

要平衡类别的话我觉得不应该仅仅根据类别数量，还应该考虑类别的难易程度。

类别数量因素：统计数据集中属于不同类别box的数量，然后计算总和，用：总和/每个类别box数量，来作为weight考虑的一个方向，相当于取了类别数量的倒数。

类别难度：AP和box大小可以作为类别难易的衡量，一般而言AP越小，越难；box越小，类别越难。我们看到，一些类别数量较少的，它的AP值也能比较大，这说明这一类是比较好识别的，特征较为显著。

 

torch.nn.functional.nll_loss(input, target, weight=**None**, size_average=**True**)

负的log likelihood损失函数. 详细请看NLLLoss（类的形式）.

参数： 

\- input - (N,C) C 是类别的个数 

\- target - (N) 其大小是 0 <= targets[i] <= C-1 

\- weight (Variable, optional) – 一个可手动指定每个类别的权重。如果给定的话，必须是大小为nclasses的Variable 

\- size_average (bool, optional) – 默认情况下，是mini-batch loss的平均值，然而，如果size_average=False，则是mini-batch loss的总和。

Variables: - weight – 对于constructor而言，每一类的权重作为输入

 

NLLLoss 的 输入 是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率. 适合网络的最后一层是log_softmax. 损失函数 nn.CrossEntropyLoss() 与 NLLLoss() 相同, 唯一的不同是它为我们去做 softmax.

NLLLoss

 ![img](https://raw.githubusercontent.com/gfjiangly/knowledges/master/assets/nll_loss.png)

即log(ak)是NLLLoss的输入

 

参考：

<https://blog.csdn.net/hao5335156/article/details/80607732>

<https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss>

<https://blog.csdn.net/u012871045/article/details/84876823>

 