

```python
# 数据集
# 自定义的数据集必须含有__getitem__和__len__函数
dataset = build_dataset(cfg)

# 网络
# 继承nn.Module，然后实现forward方法
net = build_net(cfg)
net.train()

# 加载权重
load_weights(net, cfg.pre_weights)

# 损失函数
# 继承nn.Module，然后实现forward方法
criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5, False, args.cuda)

# 优化器
optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,
                      weight_decay=args.weight_decay)
# 也可以在这儿定义学习率更新策略，把优化器作为参数，这样在迭代时就不同手动更新学习率了
# scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)
# 然后在mini-batch里用下面的代替optimizer.step()
# scheduler.step()

# 数据加载器
data_loader = data.DataLoader(dataset, args.batch_size,
                              num_workers=args.num_workers,
                              shuffle=True, collate_fn=detection_collate,
                              pin_memory=True)
# create batch iterator
batch_iterator = iter(data_loader)

# 迭代
# 也可以按照epoch参数迭代
for iteration in range(args.start_iter, cfg['max_iter']): 
    
    # load train data
    # images, targets = next(batch_iterator)
    try:
        images, targets = next(batch_iterator)
    except StopIteration:
        batch_iterator = iter(data_loader)
        images, targets = next(batch_iterator)

    # forward
    out = net(images)
    
    # 计算loss
    loss_l, loss_c = criterion(out, targets)
    loss = loss_l + loss_c

    # backprop
    # 调用backward()函数之前都要将梯度清零，因为如果梯度不清零，
    # pytorch中会将上次计算的梯度和本次计算的梯度累加。
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if iteration in cfg['lr_steps']:
        step_index += 1
        adjust_learning_rate(optimizer, args.gamma, step_index)
        print('\n adjust learning rate to %f' % cfg['lr_steps'][step_index])
        torch.save(ssd_net.state_dict(), 'weights/' +repr(iteration) + '.pth')

# 保存最终模型
torch.save(ssd_net.state_dict(), args.save_folder + '' + args.dataset + '.pth')
```

