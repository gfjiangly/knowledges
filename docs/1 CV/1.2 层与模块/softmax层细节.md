实现与公式有差别，主要考虑：

- 数值溢出，含有指数的计算要特别小心这一点；
- 温度参数

### 防溢出

由于存在指数函数exp，对于输入很大的实数会softmax数值溢出。所以在做softmax之前，需要将数据做简单的预处理，即：找出输入节点的最大值，然后让每个节点减去该最大值，使得输入节点都是小于等于0的，这样就能避免数值越界。

### 温度参数

softmax函数经常用在神经网络的最后一层，作为输出层，进行多分类。此外，softmax在增强学习领域内，softmax经常被用作将某个值转化为激活概率，这类情况下，softmax的公式如下：

![image-20200221234612999](assets/softmax%E5%B1%82%E7%BB%86%E8%8A%82/image-20200221234612999.png)

带温度参数的softmax函数

其中，T被称为是温度参数（temperature parameter）。**当T很大时，即趋于正无穷时，所有的激活值对应的激活概率趋近于相同（激活概率差异性较小）；而当T很低时，即趋于0时，不同的激活值对应的激活概率差异也就越大**。这个结论很重要，Hinton在2015年的一篇paper中重点阐释了如何根据温度参数来soften神经网络的输出，从而提出了distillation的思想和方法。

### 反向传播

softmax层的导数要分两种情况：

1）该节点是输出类别 (i  =  j);

2）该节点不为输出类别（i  !=  j）

![image-20200221234921894](assets/softmax%E5%B1%82%E7%BB%86%E8%8A%82/image-20200221234921894.png)