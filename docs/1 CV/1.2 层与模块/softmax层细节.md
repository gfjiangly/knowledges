实现与公式有差别，主要考虑：

- 数值溢出，含有指数的计算要特别小心这一点；
- 温度参数

### 防溢出

由于存在指数函数exp，对于输入很大的实数会softmax数值溢出。所以在做softmax之前，需要将数据做简单的预处理，即：找出输入节点的最大值，然后让每个节点减去该最大值，使得输入节点都是小于等于0的，这样就能避免数值越界。

### 温度参数

softmax函数经常用在神经网络的最后一层，作为输出层，进行多分类。此外，softmax在增强学习领域内，softmax经常被用作将某个值转化为激活概率，这类情况下，softmax的公式如下：

![image-20200221234612999](assets/softmax%E5%B1%82%E7%BB%86%E8%8A%82/image-20200221234612999.png)

带温度参数的softmax函数

其中，T被称为是温度参数（temperature parameter）。**当T很大时，即趋于正无穷时，所有的激活值对应的激活概率趋近于相同（激活概率差异性较小）；而当T很低时，即趋于0时，不同的激活值对应的激活概率差异也就越大**。这个结论很重要，Hinton在2015年的一篇paper中重点阐释了如何根据温度参数来soften神经网络的输出，从而提出了distillation的思想和方法。

### 反向传播

softmax层的导数要分两种情况：

1）该节点是输出类别 (i  =  j);

2）该节点不为输出类别（i  !=  j）

![image-20200221234921894](assets/softmax%E5%B1%82%E7%BB%86%E8%8A%82/image-20200221234921894.png)



## 为什么使用指数形式

直观来讲是为了拉开各类别间的差异，怎么理解呢？举个例子：假设输入的得分值为

```undefined
Z=[1, 4, 2, 3]
```

- 使用argmax函数，则输出的分类结果为

```json
[0, 1, 0, 0]
```

这个输出完美地符合实际情况。这个卵很完美，然而并没有什么用，因为argmax函数并不是处处可微的，我们无法用它来训练模型。

- 使用加权归一化函数

![img](https:////upload-images.jianshu.io/upload_images/4076491-378771207cb75f92?imageMogr2/auto-orient/strip|imageView2/2/w/148/format/webp)

输出相应的类别概率为

```json
[0.1, 0.4, 0.2, 0.3]
```

这跟argmax得出的完美的卵相差有点远。

- 引入以 e 为底的指数并加权归一化，即softmax

输出相应的类别概率为

```json
[0.032, 0.644, 0.087, 0.237]
```

这个结果将分类概率拉开了距离，富的越富，穷的越穷，最终富的消灭穷的，达到共同富裕，也就更接近期望值（就是那个完美的卵）。这主要得益于引入的指数函数。

**为什么不用2、4、10等自然数为底而要以 e 为底呢？**

因为 e 在数学界长得自然且漂亮，所以它叫自然指数。其实主要是在求导的时候足够方便，关于softmax的求导，这里就不展开了。

参考：

- https://www.jianshu.com/p/1536f98c659c