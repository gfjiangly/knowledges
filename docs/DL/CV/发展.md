Faster RCNN

anchor是如何被回归和分类的

RPN

最后输出的特征图每个点设置9个Anchor 

 ![img](https://pic3.zhimg.com/80/v2-1908feeaba591d28bee3c4a754cca282_hd.jpg) 

bbox-head: 36通道=4(box表示) * 9(anchors)

1x1卷积后输出通道数为36

cls-head: 18个通道=2(positive/negative) * 9(anchors)

1x1卷积后输出通道数为18 -> softmax，预测的anchor是前景还是背景 

这里存在正负样本不平衡问题：faster rcnn解决方案是保持正负样本1:3

Note:

​	至于为何softmax前后各有一个reshape，这与具体框架实现有关，上图针对的是caffe的（PyTorch的实现中就不需要前后的reshape）。前面的reshape便于softmax分类，后面的reshape是为了恢复形状。在caffe基本数据结构blob中以如下形式保存数据：

```text
blob=[batch_size, channel，height，width]
```

对应至上面的保存positive/negative anchors的矩阵，其在caffe blob中的存储形式为[B, 2x9, H, W]。而在softmax分类时需要进行positive/negative二分类，所以reshape layer会将其变为[B, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：

```cpp
"Number of labels must match number of predictions; "
"e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "
"label count (number of labels) must be N*H*W, "
"with integer values in {0, 1, ..., C-1}.";
```

综上所述，RPN网络中利用对anchors的分类和回归初步提取出经过回归的positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，原理类似）。

anchor作用：

- 确定哪些特征是正样本，哪些特征是负样本。体现在最后输出的特征图上，每个点有9个anchor，经过与GT的匹配，确定了哪个位置的哪些size和宽高比的anchor是正样本。正样本用于anchor回归，正负样本用于anchor分类。
- 提供回归的基准，预测基于anchor的偏移量比直接预测box位置更容易。

loss: anchor VS gt box



https://zhuanlan.zhihu.com/p/31426458



SSD

https://zhuanlan.zhihu.com/p/33544892



YOLO

https://zhuanlan.zhihu.com/p/33544892

https://blog.csdn.net/u014380165/article/details/72616238