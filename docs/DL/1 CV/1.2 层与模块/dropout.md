dropout可以让模型训练时，随机让网络的某些节点不工作（输出置零），也不更新权重(但会保存下来，下次训练得要用，只是本次训练不参与bp传播)，其他过程不变。我们通常设定一个dropout radio=p，即每个输出节点以概率p置0（不工作，权重不更新），假设每个输出都是独立的，每个输出都服从二项伯努利分布p(1-p),则大约认为训练时，只使用了(1-p)比例的输出，相当于每次训练一个子网络。测试的时候，可以直接去掉Dropout层，将所有输出都使用起来，为此需要将尺度对齐，即比例缩小输出 r=r*(1-p)。

训练的时候需要dropout，测试的时候直接去掉。

如果测试时的时候添加了dropout层，测试的时候直接把前一层的特征结果传到下一层：

dropout层相当于组合了N个网络，测试的时候去掉dropout，相当于N个网络的组合；

## 为什么有效

**我们可以把dropout理解为 模型平均**。 假设我们要实现一个图片分类任务，我们设计出了100000个网络，这100000个网络，我们可以设计得各不相同，然后我们对这100000个网络进行训练，训练完后我们采用平均的方法，进行预测，这样肯定可以提高网络的泛化能力，或者说可以防止过拟合，因为这100000个网络，它们各不相同，可以提高网络的稳定性。而所谓的**dropout我们可以这么理解，这n个网络，它们权值共享，并且具有相同的网络层数(这样可以大大减小计算量)。我们每次dropout后，网络模型都可以看成是整个网络的子网络。**

## 使用场景

当前Dropout被大量利用于全连接网络，而且一般人为设置为0.5或者0.3（[链接讲不同层代码试验](https://www.jianshu.com/p/b5e93fa01385)），而在卷积隐藏层由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因，**[Dropout策略在卷积隐藏层中使用较少](https://www.zhihu.com/question/52426832)。**drop out是用来防止过拟合的过多参数才会容易过拟合所以卷积层没必要。Ng课里也说越多参数的全链接层要drop out的概率就大一点。

## 实现细节

1. 每个神经元以p的概率输出置0；
2. 对未置0的神经元需要进行rescale：x /= p，以保持总体响应scale（期望）不变；
3. 测试时不使用