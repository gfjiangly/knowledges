主要有三种：

- unpooling：恢复pooling前的大小，恢复max位置，其余位置补0。似乎使用的不多。
- 双线性插值
- 反卷积
- 空洞卷积

### 双线性插值

特点：

- 没有参数需要学习

典型代表：YOLOv3的上采样层，FPN的上采样层

先看看`yolov3`中`upsample`的`cpu`实现：

```c
void forward_upsample_layer(const layer l, network net)
{
    fill_cpu(l.outputs*l.batch, 0, l.output, 1);
    if(l.reverse){
        upsample_cpu(l.output, l.out_w, l.out_h, l.c, l.batch, l.stride, 0, l.scale, net.input);
    }else{
        upsample_cpu(net.input, l.w, l.h, l.c, l.batch, l.stride, 1, l.scale, l.output);
    }
}

void backward_upsample_layer(const layer l, network net)
{
    if(l.reverse){
        upsample_cpu(l.delta, l.out_w, l.out_h, l.c, l.batch, l.stride, 1, l.scale, net.delta);
    }else{
        upsample_cpu(net.delta, l.w, l.h, l.c, l.batch, l.stride, 0, l.scale, l.delta);
    }
}

void upsample_cpu(float *in, int w, int h, int c, int batch, int stride, int forward, float scale, float *out)
{
    int i, j, k, b;
    for(b = 0; b < batch; ++b){
        for(k = 0; k < c; ++k){
            for(j = 0; j < h*stride; ++j){
                for(i = 0; i < w*stride; ++i){
                    int in_index = b*w*h*c + k*w*h + (j/stride)*w + i/stride;
                    int out_index = b*w*h*c*stride*stride + k*w*h*stride*stride + j*w*stride + i;
                    if(forward) out[out_index] = scale*in[in_index];
                    else in[in_index] += scale*out[out_index];
                }
            }
        }
    }
}
```

非常简单，没有需要学习的参数，甚至都使用双线性插值，直接就乘上了一个超参数比例`l.scale`（一般是均分，比如stride=2，则scale=1/4）。`l.reverse`是降采样过程标志。又想夸一夸yolo作者，darknet代码写的真好，通用性强，连cpu代码写的都这么简洁，如果我写，估计前向过程和反向过程就用两个函数实现了，作者用了一个函数实现，经验真的很重要。

再看看`PyTorch`版`FPN`中横向连接实现：

```python
def _upsample_add(self, x, y):
    """Upsample and add two feature maps.
    Args:
          x: (Variable) top feature map to be upsampled.
          y: (Variable) lateral feature map.
    Returns:
          (Variable) added feature map.
          
    Note in PyTorch, when input size is odd, the upsampled feature map
    with `F.upsample(..., scale_factor=2, mode='nearest')`
    maybe not equal to the lateral feature map size.

    e.g.
        original input size: [N,_,15,15] ->
        conv2d feature map size: [N,_,8,8] ->
        upsampled feature map size: [N,_,16,16]
        So we choose bilinear upsample which supports arbitrary output sizes.
    """

    _,_,H,W = y.size()
    return F.upsample(x, size=(H,W), mode='bilinear') + y
```

可见`upsample`用的是双线性插值，也没有要学习的参数。这个信息融合方式似乎是一个很好的改进点，如果真的有用，似乎也有很大希望结果对此敏感。
